{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Depp Neural Nets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vanishing / Exploding Gradients Problems\n",
    "\n",
    "### Xavier and He Initialization\n",
    "- normal distribution with mean 0 and standard deviation $\\sigma$\n",
    "- uniform distribution between $-r$ and $+r$\n",
    "\n",
    "### Nonsaturating Activation Functions\n",
    "- ELU (exponential linear unit)\n",
    "    - slower to compute\n",
    "    - $\\alpha = 1$\n",
    "- leaky ReLU, RReLU(randomized leaky ReLU), PReLU(parametric leaky ReLU)\n",
    "    - $\\alpha = 0.01$\n",
    "    - RReLU if overfitting\n",
    "    - PReLU if a huge training set\n",
    "- ReLU\n",
    "- tanh\n",
    "- logistic\n",
    "\n",
    "### Batch Normalization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient Clipping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ......\n",
    "\n",
    "## Faster Optimizers\n",
    "\n",
    "### Monmentum optimization\n",
    "\n",
    "### Nesterov Accelerated Gradient\n",
    "\n",
    "### AdaGrad\n",
    "\n",
    "### RMSProp\n",
    "\n",
    "### Adam Optimization\n",
    "\n",
    "### Learning Rate Scheduling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 Batch accuracy: 1.0 Validation accuracy: 0.9574\n",
      "2 Batch accuracy: 0.96 Validation accuracy: 0.9618\n",
      "4 Batch accuracy: 1.0 Validation accuracy: 0.9598\n",
      "6 Batch accuracy: 1.0 Validation accuracy: 0.9644\n",
      "8 Batch accuracy: 1.0 Validation accuracy: 0.9642\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from functools import partial\n",
    "\n",
    "tf.reset_default_graph()\n",
    "\n",
    "n_inputs = 28*28\n",
    "n_hidden1 = 300\n",
    "n_hidden2 = 100\n",
    "n_outputs = 10\n",
    "learning_rate = 0.01\n",
    "batch_size = 25\n",
    "n_epochs = 10\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape=(None, n_inputs), name=\"X\")\n",
    "y = tf.placeholder(tf.int32, shape=(None), name=\"y\")\n",
    "# training = tf.placeholder_with_default(False, shape=(), name=\"training\") # Batch Normalization\n",
    "\n",
    "with tf.name_scope(\"dnn\"):\n",
    "    \n",
    "    he_init = tf.variance_scaling_initializer()\n",
    "    hidden1 = tf.layers.dense(X, n_hidden1, activation=tf.nn.relu, kernel_initializer=he_init, name=\"hidden1\") # ReLU activation\n",
    "    # hidden1 = tf.layers.dense(X, n_hidden1, activation=tf.nn.elu, kernel_initializer=he_init, name=\"hidden1\") # ELU activation\n",
    "    # hidden1 = tf.layers.dense(X, n_hidden1, activation=tf.nn.selu, kernel_initializer=he_init, name=\"hidden1\") # SELU activation\n",
    "    hidden2 = tf.layers.dense(hidden1, n_hidden2, activation=tf.nn.relu, kernel_initializer=he_init, name=\"hidden2\")\n",
    "    logits = tf.layers.dense(hidden2, n_outputs, name=\"outputs\")\n",
    "    \n",
    "    \n",
    "    '''\n",
    "    # batch normalization\n",
    "    hidden1 = tf.layers.dense(X, n_hidden1, name=\"hidden1\")\n",
    "    bn1 = tf.layers.batch_normalization(hidden1, training=training, momentum=0.9)\n",
    "    bn1_act = tf.nn.elu(bn1)\n",
    "    \n",
    "    hidden2 = tf.layers.dense(bn1_act, n_hidden2, name=\"hidden2\")\n",
    "    bn2 = tf.layers.batch_normalization(hidden2, training=training, momentum=0.9)\n",
    "    bn2_act = tf.nn.elu(bn2)\n",
    "    \n",
    "    logits_before_bn = tf.layers.dense(bn2_act, n_outputs, name=\"outputs\")\n",
    "    logits = tf.layers.batch_normalization(logits_before_bn, training=training, momentum=0.9)\n",
    "    '''\n",
    "    \n",
    "    '''\n",
    "    # batch normalization with partial\n",
    "    my_batch_norm_layer = partial(tf.layers.batch_normalization, training=training, momentum=0.9)\n",
    "    my_dense_layer = partial(tf.layers.dense, kernel_initializer=he_init)\n",
    "    \n",
    "    hidden1 = my_dense_layer(X, n_hidden1, name=\"hidden1\")\n",
    "    bn1 = my_batch_norm_layer(hidden1)\n",
    "    bn1_act = tf.nn.elu(bn1)\n",
    "    hidden2 = my_dense_layer(bn1_act, n_hidden2, name=\"hidden2\")\n",
    "    bn2 = my_batch_norm_layer(hidden2)\n",
    "    bn2_act = tf.nn.elu(bn2)\n",
    "    logits_before_bn = my_dense_layer(bn2_act, n_outputs, name=\"outputs\")\n",
    "    logits = my_batch_norm_layer(logits_before_bn)\n",
    "    '''\n",
    "\n",
    "with tf.name_scope(\"loss\"):\n",
    "    xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y, logits=logits)\n",
    "    loss = tf.reduce_mean(xentropy, name=\"loss\")\n",
    "    \n",
    "with tf.name_scope(\"train\"):\n",
    "    # optimizer = tf.train.GradientDescentOptimizer(learning_rate) # GD\n",
    "    # optimizer = tf.train.MomentumOptimizer(learning_rate=learning_rate, momentum=0.9) # Momentum\n",
    "    # optimizer = tf.train.MomentumOptimizer(learning_rate=learning_rate, momentum=0.9, use_nesterov=True) # NAG\n",
    "    # optimizer = tf.train.AdagradOptimizer(learning_rate=learning_rate) # AdaGrad\n",
    "    # optimizer = tf.train.RMSPropOptimizer(learning_rate=learning_rate, momentum=0.9, decay=0.9, epsilon=1e-10) # RMSProp\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate) # Adam\n",
    "    training_op = optimizer.minimize(loss)\n",
    "    \n",
    "    '''\n",
    "    # learning rate scheduling\n",
    "    initial_learning_rate = 0.1\n",
    "    decay_steps = 10000\n",
    "    decay_rate = 1/10\n",
    "    global_step = tf.Variable(0, trainable=False, name=\"global_step\")\n",
    "    learning_rate = tf.train.exponential_decay(initial_learning_rate, global_step, decay_steps, decay_rate)\n",
    "    optimizer = tf.train.MomentumOptimizer(learning_rate, momentum=0.9)\n",
    "    training_op = optimizer.minimize(loss, global_step=global_step)\n",
    "    '''\n",
    "    \n",
    "    '''\n",
    "    # Gradient Clipping\n",
    "    threshold = 1.0\n",
    "    grads_and_vars = optimizer.compute_gradients(loss)\n",
    "    capped_gvs = [(tf.clip_by_value(grad, -threshold, threshold), var) for grad, var in grads_and_vars]\n",
    "    training_op = optimizer.apply_gradients(capped_gvs)\n",
    "    '''\n",
    "    \n",
    "with tf.name_scope(\"eval\"):\n",
    "    correct = tf.nn.in_top_k(logits, y, 1)\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct, tf.float32))\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "(X_train, y_train), (X_test, y_test) = tf.keras.datasets.mnist.load_data()\n",
    "X_train = X_train.astype(np.float32).reshape(-1, 28*28) / 255.0\n",
    "X_test = X_test.astype(np.float32).reshape(-1, 28*28) / 255.0\n",
    "y_train = y_train.astype(np.int32)\n",
    "y_test = y_test.astype(np.int32)\n",
    "X_valid, X_train = X_train[:5000], X_train[5000:]\n",
    "y_valid, y_train = y_train[:5000], y_train[5000:]\n",
    "\n",
    "def shuffle_batch(X, y, batch_size):\n",
    "    rnd_idx = np.random.permutation(len(X))\n",
    "    n_batches = len(X) // batch_size\n",
    "    for batch_idx in np.array_split(rnd_idx, n_batches):\n",
    "        X_batch, y_batch = X[batch_idx], y[batch_idx]\n",
    "        yield X_batch, y_batch\n",
    "\n",
    "# extra_update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS) # batch normalization\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    for epoch in range(n_epochs):\n",
    "        for X_batch, y_batch in shuffle_batch(X_train, y_train, batch_size):\n",
    "            sess.run(training_op, feed_dict={X:X_batch, y:y_batch})\n",
    "            # sess.run([training_op, extra_update_ops], feed_dict={training:True, X:X_batch, y:y_batch}) # batch normalization\n",
    "        if epoch % 2 == 0:\n",
    "            acc_batch = accuracy.eval(feed_dict={X:X_batch, y:y_batch})\n",
    "            acc_valid = accuracy.eval(feed_dict={X:X_valid, y:y_valid})\n",
    "            print(epoch, \"Batch accuracy:\", acc_batch, \"Validation accuracy:\", acc_valid)\n",
    "    save_path = saver.save(sess, \"./mymodel/my_model_final.ckpt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Reusing Pretrained Layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nfrom IPython.display import clear_output, Image, display, HTML\\ndef strip_consts(graph_def, max_const_size=32):\\n    \"\"\"Strip large constant values from graph_def.\"\"\"\\n    strip_def = tf.GraphDef()\\n    for n0 in graph_def.node:\\n        n = strip_def.node.add() \\n        n.MergeFrom(n0)\\n        if n.op == \\'Const\\':\\n            tensor = n.attr[\\'value\\'].tensor\\n            size = len(tensor.tensor_content)\\n            if size > max_const_size:\\n                tensor.tensor_content = \"<stripped %d bytes>\"%size\\n    return strip_def\\n\\ndef show_graph(graph_def, max_const_size=32):\\n    \"\"\"Visualize TensorFlow graph.\"\"\"\\n    if hasattr(graph_def, \\'as_graph_def\\'):\\n        graph_def = graph_def.as_graph_def()\\n    strip_def = strip_consts(graph_def, max_const_size=max_const_size)\\n    code = \"\"\"\\n        <script>\\n          function load() {{\\n            document.getElementById(\"{id}\").pbtxt = {data};\\n          }}\\n        </script>\\n        <link rel=\"import\" href=\"https://tensorboard.appspot.com/tf-graph-basic.build.html\" onload=load()>\\n        <div style=\"height:600px\">\\n          <tf-graph-basic id=\"{id}\"></tf-graph-basic>\\n        </div>\\n    \"\"\".format(data=repr(str(strip_def)), id=\\'graph\\'+str(np.random.rand()))\\n\\n    iframe = \"\"\"\\n        <iframe seamless style=\"width:1200px;height:620px;border:0\" srcdoc=\"{}\"></iframe>\\n    \"\"\".format(code.replace(\\'\"\\', \\'&quot;\\'))\\n    display(HTML(iframe))\\n'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "from IPython.display import clear_output, Image, display, HTML\n",
    "def strip_consts(graph_def, max_const_size=32):\n",
    "    \"\"\"Strip large constant values from graph_def.\"\"\"\n",
    "    strip_def = tf.GraphDef()\n",
    "    for n0 in graph_def.node:\n",
    "        n = strip_def.node.add() \n",
    "        n.MergeFrom(n0)\n",
    "        if n.op == 'Const':\n",
    "            tensor = n.attr['value'].tensor\n",
    "            size = len(tensor.tensor_content)\n",
    "            if size > max_const_size:\n",
    "                tensor.tensor_content = \"<stripped %d bytes>\"%size\n",
    "    return strip_def\n",
    "\n",
    "def show_graph(graph_def, max_const_size=32):\n",
    "    \"\"\"Visualize TensorFlow graph.\"\"\"\n",
    "    if hasattr(graph_def, 'as_graph_def'):\n",
    "        graph_def = graph_def.as_graph_def()\n",
    "    strip_def = strip_consts(graph_def, max_const_size=max_const_size)\n",
    "    code = \"\"\"\n",
    "        <script>\n",
    "          function load() {{\n",
    "            document.getElementById(\"{id}\").pbtxt = {data};\n",
    "          }}\n",
    "        </script>\n",
    "        <link rel=\"import\" href=\"https://tensorboard.appspot.com/tf-graph-basic.build.html\" onload=load()>\n",
    "        <div style=\"height:600px\">\n",
    "          <tf-graph-basic id=\"{id}\"></tf-graph-basic>\n",
    "        </div>\n",
    "    \"\"\".format(data=repr(str(strip_def)), id='graph'+str(np.random.rand()))\n",
    "\n",
    "    iframe = \"\"\"\n",
    "        <iframe seamless style=\"width:1200px;height:620px;border:0\" srcdoc=\"{}\"></iframe>\n",
    "    \"\"\".format(code.replace('\"', '&quot;'))\n",
    "    display(HTML(iframe))\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Avoiding Overfitting Through Regularization\n",
    "\n",
    "### Early Stopping\n",
    "\n",
    "### $l_1$ and $l_2$ Regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 Validation accuracy: 0.912\n",
      "1 Validation accuracy: 0.913\n",
      "2 Validation accuracy: 0.9156\n",
      "3 Validation accuracy: 0.918\n",
      "4 Validation accuracy: 0.92\n",
      "5 Validation accuracy: 0.9226\n",
      "6 Validation accuracy: 0.9332\n",
      "7 Validation accuracy: 0.9294\n",
      "8 Validation accuracy: 0.9308\n",
      "9 Validation accuracy: 0.9358\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from functools import partial\n",
    "\n",
    "tf.reset_default_graph()\n",
    "\n",
    "n_inputs = 28*28\n",
    "n_hidden1 = 300\n",
    "n_hidden2 = 50\n",
    "n_outputs = 10\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape=(None, n_inputs), name=\"X\")\n",
    "y = tf.placeholder(tf.int32, shape=(None), name=\"y\")\n",
    "\n",
    "scale = 0.001\n",
    "\n",
    "my_dense_layer = partial(tf.layers.dense, activation = tf.nn.relu, kernel_regularizer = tf.contrib.layers.l1_regularizer(scale))\n",
    "\n",
    "with tf.name_scope(\"dnn\"):\n",
    "    hidden1 = my_dense_layer(X, n_hidden1, name=\"hidden1\")\n",
    "    hidden2 = my_dense_layer(hidden1, n_hidden2, name=\"hidden2\")\n",
    "    logits = my_dense_layer(hidden2, n_outputs, activation = None, name=\"outputs\")\n",
    "\n",
    "with tf.name_scope(\"loss\"):\n",
    "    xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y, logits=logits)\n",
    "    base_loss = tf.reduce_mean(xentropy, name=\"avg_xentropy\")\n",
    "    reg_losses = tf.get_collection(tf.GraphKeys.REGULARIZATION_LOSSES)\n",
    "    loss = tf.add_n([base_loss] + reg_losses, name=\"loss\")\n",
    "\n",
    "with tf.name_scope(\"eval\"):\n",
    "    correct = tf.nn.in_top_k(logits, y, 1)\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct, tf.float32), name=\"accuracy\")\n",
    "\n",
    "learning_rate = 0.01\n",
    "\n",
    "with tf.name_scope(\"train\"):\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "    training_op = optimizer.minimize(loss)\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "n_epochs = 10\n",
    "batch_size = 25\n",
    "\n",
    "(X_train, y_train), (X_test, y_test) = tf.keras.datasets.mnist.load_data()\n",
    "X_train = X_train.astype(np.float32).reshape(-1, 28*28) / 255.0\n",
    "X_test = X_test.astype(np.float32).reshape(-1, 28*28) / 255.0\n",
    "y_train = y_train.astype(np.int32)\n",
    "y_test = y_test.astype(np.int32)\n",
    "X_valid, X_train = X_train[:5000], X_train[5000:]\n",
    "y_valid, y_train = y_train[:5000], y_train[5000:]\n",
    "\n",
    "def shuffle_batch(X_train, y_train, batch_size):\n",
    "    rnd_idx = np.random.permutation(len(X_train))\n",
    "    n_batches = len(X_train) // batch_size\n",
    "    for batch_idx in np.array_split(rnd_idx, n_batches):\n",
    "        X_batch, y_batch = X_train[batch_idx], y_train[batch_idx]\n",
    "        yield X_batch, y_batch\n",
    "    \n",
    "\n",
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    for epoch in range(n_epochs):\n",
    "        for  X_batch, y_batch in shuffle_batch(X_train, y_train, batch_size):\n",
    "            sess.run(training_op, feed_dict={X:X_batch, y:y_batch})\n",
    "        accuracy_val = accuracy.eval(feed_dict={X:X_valid, y:y_valid})\n",
    "        print(epoch, \"Validation accuracy:\", accuracy_val)\n",
    "    \n",
    "    save_path = saver.save(sess, \"./mymodel/my_model_final.ckpt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 Validation accuracy: 0.8704\n",
      "1 Validation accuracy: 0.9002\n",
      "2 Validation accuracy: 0.9174\n",
      "3 Validation accuracy: 0.925\n",
      "4 Validation accuracy: 0.9342\n",
      "5 Validation accuracy: 0.9392\n",
      "6 Validation accuracy: 0.945\n",
      "7 Validation accuracy: 0.9484\n",
      "8 Validation accuracy: 0.9508\n",
      "9 Validation accuracy: 0.9538\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from functools import partial\n",
    "\n",
    "tf.reset_default_graph()\n",
    "\n",
    "n_inputs = 28*28\n",
    "n_hidden1 = 300\n",
    "n_hidden2 = 50\n",
    "n_outputs = 10\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape=(None, n_inputs), name=\"X\")\n",
    "y = tf.placeholder(tf.int32, shape=(None), name=\"y\")\n",
    "\n",
    "training = tf.placeholder_with_default(False, shape=(), name=\"training\")\n",
    "dropout_rate = 0.5\n",
    "\n",
    "X_drop = tf.layers.dropout(X, dropout_rate, training=training)\n",
    "\n",
    "with tf.name_scope(\"dnn\"):\n",
    "    hidden1 = tf.layers.dense(X_drop, n_hidden1, activation=tf.nn.relu, name=\"hidden1\")\n",
    "    hidden1_drop = tf.layers.dropout(hidden1, dropout_rate, training=training)\n",
    "    hidden2 = tf.layers.dense(hidden1_drop, n_hidden2, activation=tf.nn.relu, name=\"hidden2\")\n",
    "    hidden2_drop = tf.layers.dropout(hidden2, dropout_rate, training=training)\n",
    "    logits = tf.layers.dense(hidden2_drop, n_outputs, name=\"outputs\")\n",
    "\n",
    "with tf.name_scope(\"loss\"):\n",
    "    xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y, logits=logits)\n",
    "    loss = tf.reduce_mean(xentropy, name=\"loss\")\n",
    "\n",
    "with tf.name_scope(\"eval\"):\n",
    "    correct = tf.nn.in_top_k(logits, y, 1)\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct, tf.float32), name=\"accuracy\")\n",
    "\n",
    "learning_rate = 0.01\n",
    "\n",
    "with tf.name_scope(\"train\"):\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "    training_op = optimizer.minimize(loss)\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "n_epochs = 10\n",
    "batch_size = 25\n",
    "\n",
    "(X_train, y_train), (X_test, y_test) = tf.keras.datasets.mnist.load_data()\n",
    "X_train = X_train.astype(np.float32).reshape(-1, 28*28) / 255.0\n",
    "X_test = X_test.astype(np.float32).reshape(-1, 28*28) / 255.0\n",
    "y_train = y_train.astype(np.int32)\n",
    "y_test = y_test.astype(np.int32)\n",
    "X_valid, X_train = X_train[:5000], X_train[5000:]\n",
    "y_valid, y_train = y_train[:5000], y_train[5000:]\n",
    "\n",
    "def shuffle_batch(X_train, y_train, batch_size):\n",
    "    rnd_idx = np.random.permutation(len(X_train))\n",
    "    n_batches = len(X_train) // batch_size\n",
    "    for batch_idx in np.array_split(rnd_idx, n_batches):\n",
    "        X_batch, y_batch = X_train[batch_idx], y_train[batch_idx]\n",
    "        yield X_batch, y_batch\n",
    "    \n",
    "\n",
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    for epoch in range(n_epochs):\n",
    "        for  X_batch, y_batch in shuffle_batch(X_train, y_train, batch_size):\n",
    "            sess.run(training_op, feed_dict={X:X_batch, y:y_batch, training:True})\n",
    "        accuracy_val = accuracy.eval(feed_dict={X:X_valid, y:y_valid})\n",
    "        print(epoch, \"Validation accuracy:\", accuracy_val)\n",
    "    \n",
    "    save_path = saver.save(sess, \"./mymodel/my_model_final.ckpt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Max-Norm Regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 Validation accuracy: 0.9246\n",
      "1 Validation accuracy: 0.9394\n",
      "2 Validation accuracy: 0.9514\n",
      "3 Validation accuracy: 0.9584\n",
      "4 Validation accuracy: 0.9618\n",
      "5 Validation accuracy: 0.9648\n",
      "6 Validation accuracy: 0.9676\n",
      "7 Validation accuracy: 0.9696\n",
      "8 Validation accuracy: 0.973\n",
      "9 Validation accuracy: 0.9714\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from functools import partial\n",
    "\n",
    "tf.reset_default_graph()\n",
    "\n",
    "n_inputs = 28*28\n",
    "n_hidden1 = 300\n",
    "n_hidden2 = 50\n",
    "n_outputs = 10\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape=(None, n_inputs), name=\"X\")\n",
    "y = tf.placeholder(tf.int32, shape=(None), name=\"y\")\n",
    "\n",
    "with tf.name_scope(\"dnn\"):\n",
    "    hidden1 = tf.layers.dense(X, n_hidden1, activation=tf.nn.relu, name=\"hidden1\")\n",
    "    hidden2 = tf.layers.dense(hidden1, n_hidden2, activation=tf.nn.relu, name=\"hidden2\")\n",
    "    logits = tf.layers.dense(hidden2, n_outputs, name=\"outputs\")\n",
    "\n",
    "with tf.name_scope(\"loss\"):\n",
    "    xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y, logits=logits)\n",
    "    loss = tf.reduce_mean(xentropy, name=\"loss\")\n",
    "\n",
    "with tf.name_scope(\"eval\"):\n",
    "    correct = tf.nn.in_top_k(logits, y, 1)\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct, tf.float32), name=\"accuracy\")\n",
    "\n",
    "learning_rate = 0.01\n",
    "\n",
    "with tf.name_scope(\"train\"):\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "    training_op = optimizer.minimize(loss)\n",
    "    \n",
    "threshold = 1.0\n",
    "weights = tf.get_default_graph().get_tensor_by_name(\"hidden1/kernel:0\")\n",
    "clipped_weights = tf.clip_by_norm(weights, clip_norm=threshold, axes=1)\n",
    "clip_weights = tf.assign(weights, clipped_weights)\n",
    "\n",
    "weights2 = tf.get_default_graph().get_tensor_by_name(\"hidden2/kernel:0\")\n",
    "clipped_weights2 = tf.clip_by_norm(weights2, clip_norm=threshold, axes=1)\n",
    "clip_weights2 = tf.assign(weights2, clipped_weights2)\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "n_epochs = 10\n",
    "batch_size = 25\n",
    "\n",
    "(X_train, y_train), (X_test, y_test) = tf.keras.datasets.mnist.load_data()\n",
    "X_train = X_train.astype(np.float32).reshape(-1, 28*28) / 255.0\n",
    "X_test = X_test.astype(np.float32).reshape(-1, 28*28) / 255.0\n",
    "y_train = y_train.astype(np.int32)\n",
    "y_test = y_test.astype(np.int32)\n",
    "X_valid, X_train = X_train[:5000], X_train[5000:]\n",
    "y_valid, y_train = y_train[:5000], y_train[5000:]\n",
    "\n",
    "def shuffle_batch(X_train, y_train, batch_size):\n",
    "    rnd_idx = np.random.permutation(len(X_train))\n",
    "    n_batches = len(X_train) // batch_size\n",
    "    for batch_idx in np.array_split(rnd_idx, n_batches):\n",
    "        X_batch, y_batch = X_train[batch_idx], y_train[batch_idx]\n",
    "        yield X_batch, y_batch\n",
    "    \n",
    "\n",
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    for epoch in range(n_epochs):\n",
    "        for  X_batch, y_batch in shuffle_batch(X_train, y_train, batch_size):\n",
    "            sess.run(training_op, feed_dict={X:X_batch, y:y_batch})\n",
    "            clip_weights.eval()\n",
    "            clip_weights2.eval()\n",
    "        accuracy_val = accuracy.eval(feed_dict={X:X_valid, y:y_valid})\n",
    "        print(epoch, \"Validation accuracy:\", accuracy_val)\n",
    "    \n",
    "    save_path = saver.save(sess, \"./mymodel/my_model_final.ckpt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Augmentation"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
